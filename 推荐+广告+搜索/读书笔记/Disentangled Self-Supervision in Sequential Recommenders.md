---
alias: 
å‘å¸ƒæ—¶é—´: 2020-01-01
å‡ºå“æ–¹: é˜¿é‡Œ
ä»·å€¼: â­â­
ç¯èŠ‚: 
---
å…³é”®è¯:: #ML/è‡ªç›‘ç£å­¦ä¹ 

---
# Disentangled Self-Supervision in Sequential Recommenders

* ä¸€èˆ¬éƒ½æ˜¯seq2itemï¼Œå°±æ˜¯æ‹¿ä¹‹å‰çš„ç”¨æˆ·å†å²ï¼Œé¢„æµ‹next clickedï¼Œç¼ºç‚¹æ˜¯è¿‘è§†çš„ã€‚
* è¿™ç¯‡æ–‡ç« æ”¹æˆseq2seqï¼Œé¢„æµ‹long-term futureã€‚ä½†æ˜¯seq2item --> seq2seqä¹Ÿæ˜¯æœ‰éš¾ç‚¹çš„
  * æ„å»ºæ›´é•¿çš„futureï¼Œè‚¯å®šæ¯”a single next itemè¦éš¾, and inefficientã€‚å…¶ä¸­ä¸€ä¸ªåŸå› ï¼Œå¾ˆå¤šé¢„æµ‹æ˜¯é‡å¤çš„ï¼Œæ¯”å¦‚future sequenceä¸­çš„å¾ˆå¤šiteméƒ½æ¥æºäºuser historyæ‰€åæ˜ çš„åŒä¸€ä¸ªintentionã€‚
  * å³ä½¿æ„å»ºäº†æ›´é•¿çš„future sequenceï¼Œè¿™å…¶ä¸­æ‰€åŒ…å«çš„ç”¨æˆ·çš„æŸäº›å…´è¶£ç‚¹intentionï¼Œå¯èƒ½æ˜¯ç”±ä¹‹å‰çš„ç”¨æˆ·è¡Œä¸ºå†å²æ‰€æ— æ³•è§£é‡Šçš„

* ä¸ºäº†å…‹æœä»¥ä¸Šseq2seqå¸¦æ¥çš„å›°éš¾ï¼Œæ–‡ç« æå‡ºä¸¤ä¸ªåˆ›æ–°
  * ä¸ºäº†è§£å†³[one by one predictçš„ä½æ•ˆé—®é¢˜](siyuan://blocks/20211115182146-02u2wuf)ï¼Œå¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ï¼Œæ„å»ºæ•´ä¸ªfuture sequenceçš„è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¸ºfuture sequenceä¸­çš„æ¯ä¸ªitemå•ç‹¬è¡¨ç¤º
  * ä¸ºäº†è§£å†³[future intentionå¯èƒ½ä¸å†å²æ— å…³çš„é—®é¢˜](siyuan://blocks/20211115182146-707uu1d)ï¼Œwe can identify which portion of the future sequence is predictable from the earlier behaviors, then construct seq2seq training samples using only pairs of sub-sequences that involve a shared intention.

## Sequence-to-Sequence Self-Supervision

* our seq2seq training strategy asks the model to predict the representation of the future sub-sequence given the earlier sequenceâ€™s representations.
* This design avoids individually reconstructing all the behaviors in the future sequence and eases convergence of the seq2seq training process.
* The representation to be predicted effectively serves as a distilled pseudo behavior (e.g., clicking a pseudo item) in the vector space, which summarizes the main intention present in the future sequence.

### Contrastive Learning Loss

æ€è·¯æ˜¯ä¸==ç™¾åº¦çš„â€œå­ªç”Ÿç½‘ç»œâ€éå¸¸ç›¸ä¼¼ï¼Œå°±æ˜¯ç”¨æˆ·çš„ä¸€éƒ¨åˆ†å†å²ï¼Œåº”è¯¥ä¸ä»–è‡ªå·±çš„å¦ä¸€åŠå†å²ï¼Œæ˜¯ç›¸ä¼¼çš„==ã€‚

* We construct the mini-batch B by **sampling** each of its element from the training set {(ğ‘¢,ğ‘¡) : 1 â‰¤ ğ‘¢ â‰¤ ğ‘, 1 â‰¤ ğ‘¡ â‰¤ ğ‘‡ ğ‘¢ âˆ’ 1} uniformly.
* Each training example (ğ‘¢,ğ‘¡) in the mini-batch B refers to an earlier sequence $x_{1:t}^{(u)}$ and its corresponding future sequence $x_{t+1:T_u}^{(u)}$ã€‚
	* åªä¸è¿‡ï¼Œä½œè€…åœ¨embedding user sequenceæ—¶è¿˜è€ƒè™‘äº†æ—¶é—´å› ç´ ï¼Œæ‰€ä»¥è®¤ä¸º$x_{1:t}^{(u)}$çš„å‘é‡è¡¨ç¤ºï¼Œåº”è¯¥ä¸reversed future sequence, $x_{T_u:t+1}^{(u)}$çš„å‘é‡è¡¨ç¤ºï¼Œç›¸ä¼¼ã€‚
* å¦å¤–ä¸å¸¸è§„ä½œæ³•ä¸åŒçš„æ˜¯ï¼Œä¸ºäº†åæ˜ å¤šå…´è¶£ï¼Œä½œè€…æ‰€ä½¿ç”¨çš„â€œuser sequence --> vectorâ€çš„æ˜ å°„å‡½æ•°ï¼Œä¸æ˜¯åªè¿”å›ä¸€ä¸ªå‘é‡ï¼Œè€Œæ˜¯è¿”å›Kä¸ªå‘é‡ã€‚
	* ä¸ºæ­¤ï¼Œä¹Ÿå°†lossä¿®æ­£ä¸ºï¼Œåœ¨ç¬¬kä¸ªå…´è¶£çš„éšå‘é‡ç©ºé—´å†…ï¼Œç”¨æˆ·å‰ä¸€éƒ¨åˆ†å†å²çš„å‘é‡è¡¨ç¤ºï¼Œä¸åä¸€æ®µå†å²çš„å‘é‡è¡¨ç¤ºï¼Œåº”è¯¥ç›¸ä¼¼ã€‚

![image.png | 500](assets/image-20211116205356-gt507e6.png)


è¿™é‡Œè¿˜åŠ äº†ä¸€ä¸ª**å°trickï¼Œå°½å¿«æˆ‘è®¤ä¸ºæ˜¯ä¸€ä¸ªä¸­çœ‹ä¸ä¸­ç”¨çš„trick**ï¼Œè¿™é‡Œçš„user "u", t (å‰åå†å²çš„æ—¶é—´åˆ†è£‚ç‚¹), kï¼ˆå…´è¶£ï¼‰çš„ç»„åˆå¤ªå¤šäº†ã€‚ä½œè€…è®¤ä¸ºï¼Œâ€œif an earlier sequence x 1:ğ‘¡ involves intention under latent (ğ‘¢) category ğ‘˜ = 1 and ğ‘˜ = 3 while the future sequence x ğ‘‡ ğ‘¢ :ğ‘¡+1 involves intention under category ğ‘˜ = 1 and ğ‘˜ = 2, then we should use only L ğ‘ 2ğ‘  (ğœ½,ğ‘¢,ğ‘¡,ğ‘˜ = 1), but not use ğ‘˜ = 2 and ğ‘˜ = 3, "ã€‚ä½†æ˜¯ï¼Œ**ä½œè€…æ²¡æœ‰é‡‡ç”¨è§„åˆ™æ¥é™åˆ¶è¦è®­ç»ƒçš„æ ·æœ¬ï¼Œå› æ­¤ä¸€ç‚¹è®¡ç®—é‡ä¹ŸèŠ‚çœä¸ä¸‹æ¥**ã€‚è€Œæ˜¯é‡‡ç”¨äº†åœ¨lossä¸ŠåŠ maskè¿™æ ·ç¨€å¥‡å¤æ€ªçš„æ–¹å¼ï¼š è¿™é‡Œçš„$\tau$ä»£è¡¨top nå°çš„loss, næ˜¯è¶…å‚ã€‚ä¹Ÿå°±æ˜¯è¯´we keep only the top ğœ†-percent of the sequenceto-sequence training samples that are considered to be of high confidence by the model.

![image.png | 400](assets/image-20211116205909-kuyx707.png)


### Total Loss

ä¹‹å‰å¸¸ç”¨çš„æ˜¯seq2item taskï¼Œä¹Ÿå°±æ˜¯åˆ©ç”¨ä¹‹å‰çš„å†å²ï¼Œé¢„æµ‹next clickedã€‚è¿™é‡Œå¼•å…¥seq2seq, complement, not replace, the traditional seq2item loss. In other words, we minimize **both the seq2item loss and the seq2seq loss** when processing each mini-batch. è¿™å’Œcontrastive learningçš„å¸¸è§„ä½œæ³•ä¹Ÿæ˜¯ç¬¦åˆçš„ï¼Œ**contrastive learningä¸€èˆ¬éƒ½æ˜¯ä½œä¸ºè¾…åŠ©è®­ç»ƒçš„è§’è‰²è€Œå­˜åœ¨çš„**ã€‚![image.png | 300](assets/image-20211116210647-8v9alex.png)

$L_{s2i}$æ˜¯ä¸»è®­ç»ƒä»»åŠ¡

* $x_{1:t}^{(u)}$ä»£è¡¨user "u"åœ¨tæ—¶é—´ä¹‹å‰çš„å†å²åºåˆ—
* $\phi_{\theta}^{k}$jæ˜¯å°†ç”¨æˆ·å†å²å‹ç¼©æˆä¸€ä¸ªå…´è¶£å‘é‡ã€‚è¿™é‡Œé‡‡ç”¨äº†å¤šå…´è¶£ï¼Œæ‰€ä»¥è¾“å…¥ä¸€ä¸ªç”¨æˆ·å†å²ï¼Œè¿”å›çš„ä¸æ˜¯ä¸€ä¸ªå‘é‡ï¼Œè€Œæ˜¯Kä¸ªå‘é‡
* $h_{t+1}^u$jæ˜¯ç”¨æˆ·åœ¨ä¸‹ä¸€æ—¶åˆ»t+1ç‚¹å‡»çš„itemçš„embeddingï¼Œæ¥è‡ªä¸€ä¸ªDç»´çš„embedding matrix

![image.png | 400](assets/image-20211116203232-z95vzj0.png)


## Disentangled Sequence Encoding

* Our second core idea is to design a sequence encoder that can infer and disentangle the latent intentions reflected by a given sequence of behaviors.
* The disentangled encoder outputs multiple representations of a given sequence of behaviors, where each representation focuses on a distinct sub-sequence of the given sequence.
* Each of the multiple representations characterizes the userâ€™s intention related to a different latent category.
* We can then construct seq2seq training samples using only pairs of sub-sequences whose intentions are relevant in that they involve the same latent category

è®©è¾“å…¥çš„ä¸€ä¸ªsequenceèƒ½å¤Ÿè¿”å›**multiple intentionï¼Œæœ€ç›´è§‚çš„æƒ³æ³•å°±æ˜¯ä½¿ç”¨multi-head transformer**ï¼Œä½†æ˜¯å¥½åƒæ•ˆæœä¸æ˜¯å¾ˆå¥½ï¼Œ**å®éªŒè¯æ˜multi-headä¸single-headç›¸æ¯”ï¼Œå¹¶æœªå¸¦æ¥æ˜¾è‘—æå‡**ã€‚

ä½œè€…æ‰€é‡‡ç”¨çš„disentangleçš„æ–¹æ³•æ˜¯ï¼š

1. åªé‡‡ç”¨single-head transformeræ¥å¤„ç†sequenceï¼Œå°†sequenceä¸­çš„æ¯ä¸ªclicked itemæ˜ å°„æˆ![image.png | 200](assets/image-20211117113958-3aobw7b.png)
2. **å®šä¹‰Kä¸ªå…´è¶£å‘é‡ï¼Œæˆä¸ºtrainable variableçš„ä¸€éƒ¨åˆ†**
3. è®¡ç®—ç¬¬iä¸ªæ—¶åˆ»çš„sequence item embeddingåˆ°æ¯ä¸ªintentionçš„æƒé‡ï¼ˆhow likely the primary intention at position ğ‘– is related with the ğ‘˜ th latent categoryï¼‰

    ![image.png | 300](assets/image-20211117114428-nw0xrvi.png)
4. å†è®¡ç®—how likely the primary intention at position ğ‘– is important for predicting the userâ€™s future intentions. çœ‹ç€å…¬å¼å¾ˆå¤æ‚ï¼Œ<span style="color:orange;font-weight:bold">å…¶å®å°±ç›¸å½“äºæ‹¿sequenceçš„æœ€åä¸€ä¸ªå…ƒç´ ä¸ºqueryï¼Œå¯¹åºåˆ—ä¸­æ‰€æœ‰å…ƒç´ åšattention</span>ã€‚

    ![image.png | 300](assets/image-20211117114642-rx0h51j.png)
5. æœ€åç”¨æˆ·åºåˆ—$x_{1:t}^{(u)}$åœ¨ç¬¬Kä¸ªintentionä¸Šçš„å‘é‡è¡¨ç¤ºï¼Œå°±æ˜¯åºåˆ—ä¸­æ¯ä¸ªå…ƒç´ çš„å‘é‡è¡¨ç¤ºï¼Œè€ƒè™‘äº†å¯¹ç¬¬kä¸ªå…´è¶£çš„ç›¸ä¼¼åº¦ï¼Œå†è€ƒè™‘äº†å…¶å¯¹æœªæ¥é¢„æµ‹èƒ½åŠ›ï¼Œä¹‹åçš„åŠ æƒå’Œ

![image.png | 300](assets/image-20211117114830-62yhimt.png)


å¦å¤–ï¼Œå¸¸è§çš„disentangleéœ€è¦æœ‰regularizationå¼ºè¿«æ¯ä¸ªç”¨æˆ·å­¦åˆ°çš„Kä¸ªintentionä¹‹é—´äº’ä¸ç›¸åŒã€‚è€ƒè™‘åˆ°[seq2seq contrastive lossä¸­çš„åˆ†æ¯](siyuan://blocks/20211116204511-k0myurs)å·²ç»è€ƒè™‘äº†å…¶ä»–K-1ç§å…´è¶£ï¼Œè¿™é‡Œå°±ä¸åŠ äº†ã€‚