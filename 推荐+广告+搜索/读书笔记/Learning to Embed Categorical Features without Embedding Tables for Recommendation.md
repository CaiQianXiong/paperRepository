---
alias: 
å‘å¸ƒæ—¶é—´: 2021-01-01
å‡ºå“æ–¹: Google
ä»·å€¼: â­
ç¯èŠ‚: 
---
å…³é”®è¯:: #Reco/Embedding 

---
# Learning to Embed Categorical Features without Embedding Tables for Recommendation
==æ¦‚å¿µç©å¾—æŒºå¥½ï¼Œæ–‡ç« ä¸­è‡ªå·±éƒ½æ‰¿è®¤ï¼Œæœ‰dnnå–ä»£embedding tableä¸å®¹æ˜“è®­ç»ƒå¥½==

## æ¦‚è¿°
- ä¼ ç»ŸEmbeddingå­˜åœ¨çš„é—®é¢˜
	- Huge vocabulary size
	- å­—å…¸æ˜¯åŠ¨æ€å˜åŒ–çš„ï¼š
		- New users and new items enter the system on a daily basis, ä»è€Œé€ æˆ**OOV**é—®é¢˜ 
		- stale items are gradually vanishing.
	- æ•°æ®æ˜¯ä¸¥é‡ä¸å‡è¡¡çš„ï¼šsmall number of training examples on infrequent feature valuesï¼Œè¿™äº›ç‰¹å¾çš„embeddingæ˜¯å­¦ä¸å¥½çš„
- Embedding Table: å¯ä»¥çœ‹æˆ1-layer wide NN
- æå‡ºè§£å†³æ–¹æ³•ä»£æ›¿Embedding Table
	- we seek to explore a deep, narrow, and collision-free embedding scheme without using embedding tables. 
	- We propose the Deep Hash Embedding (DHE) approach, that uses dense encodings and a **deep embedding network** to compute embeddings on the fly. 
		- we use *multiple hashing* and appropriate transformations to generate a *unique, deterministic, dense, and real-valued vector* as the identifier encoding of the given feature value, 
			- è§£å†³äº†huge vocabulary sizeçš„é—®é¢˜ 
		- and then the deep embedding network transforms the encoding to the final feature embeddings.

## ä¼ ç»ŸåŸºäºEmbedding Tableçš„æ–¹æ³•
- hash-trickæ˜¯è§£å†³huge vocabulary sizeçš„ä¸€ä¸ªæ–¹æ³•ï¼Œä½†æ˜¯ä»ç„¶æ— æ³•é¿å…collision
- ä¸€ä¸ªæ–¹æ³•ï¼Œå°±æ˜¯æ¯ä¸ªç‰¹å¾ä¸å†åªåšä¸€æ¬¡hash
	- The core idea is that the concatenated encodings are less likely to be collided. 
	- We can lookup **ğ‘˜ embeddings in ğ‘˜ embedding tables (respectively)** and **aggregate** them into the final embedding
	- å¯ä»¥å¦‚ä¸Šç”¨kä¸ªç‹¬ç«‹çš„embedding tableï¼Œå¯ä»¥kæ¬¡hashè¿˜æ˜¯ä½¿ç”¨ä¸€ä¸ª**å…±äº«çš„embedding table**
	- A common aggregation approach is â€˜addâ€™
	- ä»£ä»·è¿˜æ˜¯æ¯”è¾ƒå¤§çš„ï¼ˆç‰¹åˆ«æ˜¯ç‹¬ç«‹embedding tableï¼‰ï¼Œä¸€èˆ¬Kå–å€¼è¾ƒå°ï¼Œä¸€èˆ¬=2 ^73234c

## DEEP HASH EMBEDDINGS (DHE)
ä¸ä¼ ç»Ÿæ–¹æ³•æ¯”è¾ƒ
- embedding table
	- Encodingè¿‡ç¨‹ï¼šå°†feature idæ˜ å°„æˆä¸€ä¸ªsparse one-hot encodingï¼Œå”¯ä¸€çš„value=1è¿˜æ˜¯ä¸€ä¸ªæ•´æ•°
	- Embeddingè¿‡ç¨‹ï¼šembedding tableç›¸å½“äºä¸€ä¸ª1-layer shallow network
- DHE
	- Encodingè¿‡ç¨‹ï¼šé€šè¿‡multiple-hashingï¼Œå°†ä¸€ä¸ªfeature idæ˜ å°„æˆä¸€ä¸ª**real-valued dense** vector
	- Embeddingè¿‡ç¨‹ï¼šä¸å†æ˜¯shallowï¼Œè€Œæ˜¯ç”¨deep NNæ¥åšembedding

### Dense Hash Encoding
- Step1: ğ¸ : N â†’ $R^k$ uses **ğ‘˜ universal hash functions** to map a feature value to a ğ‘˜-dimensional dense but integer encodings.
	- $E^{\prime}(s)$=$[H^{(1)}s,H^{(2)}s,...,H^{(k)}s,]$
	- $H^{(1)}: N \rightarrow {1,2,...,m}$ï¼Œmæ˜¯ä¸€ä¸ªéå¸¸å¤§çš„æ•°ï¼Œæ¯”å¦‚$10^6$
	- [[ç‰¹å¾äº¤äº’#^73234c | ä¸double-hashingä¸åŒ]]ï¼Œè¿™é‡Œkå–å¾—è¶Šå¤§è¶Šå¥½ã€‚==å¥½åƒè§£å†³äº†hash collisionçš„é—®é¢˜==ã€‚
- Step2: $E^{\prime}(s)$æ˜¯å…¨éƒ¨ç”±æ•´æ•°ç»„æˆçš„å‘é‡ï¼Œä¸é€‚åˆå–‚å…¥dnnï¼Œéœ€è¦åšè½¬æ¢
	- å¯ä»¥åšuniform normalizationåˆ°\[-1,1\]ã€‚
	- åœ¨uniform normalizationä¹‹åï¼Œå¯ä»¥å†standard normalizeåˆ°N(0,1)
	- æ ¹æ®ä½œè€…çš„ç»éªŒï¼Œuniform normalizationå°±å·²ç»è¶³å¤Ÿäº†ï¼Œæ²¡å¿…è¦å†åšåé¢çš„standard normalizationã€‚
- æ³¨æ„ï¼š
	- k hashing functionæ˜¯**deterministic and non-learnable**ï¼Œä¸åƒembedding tableé‚£æ ·æ˜¯è¦å­¦ä¹ çš„å‚æ•°
	- ä¸embedding tableä¸åŒï¼Œå°½ç®¡è¿™é‡Œä¹Ÿå°†feature idè½¬åŒ–ä¸ºä¸€ä¸ªkç»´çš„real-valued dense vectorï¼Œä½†æ˜¯transform on the flyï¼Œè€Œä¸åƒembedding tableé‚£ä¹ˆè¦å æ®ç©ºé—´
	- k hashingå¯ä»¥å¹¶å‘è¿›è¡Œ

### Deep Embedding Network
![[Pasted image 20220226150051.png | 400]]
æ‰€è°“çš„embedding networkä¸å¸¸è§„çš„dnnæ²¡æœ‰ä»»ä½•åŒºåˆ«ã€‚
We transform the ğ‘˜-dim encoding via â„ hidden layers with DNN nodes. Then, the outputs layer (with ğ‘‘ nodes) transforms the last hidden layer to the ğ‘‘-dim feature value embedding.
However, we found that ==training the deep embedding network is quite challenging (in contrast, one-hot based shallow networks are much easier to train). We observed inferior training and testing performance==.
However, we found the embedding network is ==underfitting instead of overfitting== in our case, as the embedding generation task requires highly non-linear transformations from hash encodings to embeddings.
- We suspect the default ReLU activation is not expressive enough, as ReLU networks are piece-wise linear functions
- We found the recently proposed Mish activation $ğ‘“ (ğ‘¥)=ğ‘¥ Â· tanh(ln(1 + ğ‘’ ğ‘¥ ))$ consistently performs better than ReLU and others. 
- We also found batch normalization (BN) can stabilize the training and achieve better performance. 
- However, regularization methods like dropout are not beneficial,

### Side Features Enhanced Encodings for Generalization
ä¼ ç»Ÿçš„embedding table not imply any inherent similarity between different IDs. å¯ä»¥ç”¨side featureæ¥improve generalizationï¼Œä½†æ˜¯é‚£ä¸€èˆ¬éƒ½æ˜¯ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸æ˜¯ç”¨äºæå‡embeddingçš„è´¨é‡ã€‚
ï¼Ÿï¼Ÿï¼Ÿä½†æ˜¯é—®é¢˜åœ¨äºï¼Œä½ çš„è¿™ç§æ–¹æ³•è¿˜åŒºåˆ†å¾—å‡ºï¼Œå“ªé‡Œæ˜¯åœ¨è®­ç»ƒembeddingï¼Œå“ªé‡Œæ˜¯åœ¨è®­ç»ƒmodelå—ï¼Ÿæ‰€è°“çš„embedding networkä¸æ˜¯å’Œmodelæ˜¯ä¸€æ ·çš„å—ï¼Ÿï¼Ÿï¼Ÿ

One straightforward way to enhance the encoding is 
- directly concatenating the generalizable features and the hash encodings. è¿™ç§ä½œæ³•æ˜¯å°†side featureå–‚å…¥dnnæœ‰å•¥åŒºåˆ«ï¼Ÿï¼Ÿï¼Ÿ
- The enhanced encoding is then fed into the deep embedding network for embedding generation