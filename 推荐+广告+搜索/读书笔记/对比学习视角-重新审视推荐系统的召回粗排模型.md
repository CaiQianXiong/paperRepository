---
alias:
发布时间: 
出品方: 新浪 
价值: ⭐⭐⭐
---

环节:: 粗排
关键词:: #ML/自监督学习 

---

# 张俊林 [对比学习视角:重新审视推荐系统的召回粗排模型](https://zhuanlan.zhihu.com/p/424198603) #TODO 

## 对比学习基础

对比学习，以一个图像为例子，

* 通过自动构造正例或负例，形成图片的两个view,通过encoder把它们编码，即将输入映射到一个投影空间里。
* 对比学习的优化目标是：希望投影空间里两个正例的距离比较近，如果是负例，则希望它们的距离远一些

可以看到，与召回算法的思维是高度一致的。


好的对比学习应兼顾两个要素：**Alignment和Uniformity**。

* Alignment代表我们希望对比学习把相似的正例在投影空间里面有相近的编码，一般我们做一个embedding映射系统，都是希望达成此目标。
* Uniformity直观上来说就是：当所有实例映射到投影空间之后，我们希望它在投影空间内的分布是尽可能均匀的。它实际想达成的是:我们希望实例映射到投影空间后，在对应的Embedding包含的信息里，可以更多保留自己个性化的与众不同的信息。**引入负例，并让负例embedding相距越远越好，就是达到uniformity的一种手段**。

  * 反例就是：“**模型坍塌**”。就是说不论你输入的是什么图片，经过映射函数之后，在投影空间里面，所有图像的编码都会坍塌到同一个点。坍塌到同一个点又是什么含义呢？就是说不论我的输入是什么，最终经过函数映射，被映射成同一个embedding，所有图像对应的Embedding都是一样的。


## CV经典对比学习算法

### 思路框架

构建对比学习算法，==最关键的是三个问题==：

1. 第一个问题是：正例怎么构造？对于对比学习来说，原则上正例应该是自动构造出的，也就是自监督的方式构造的。负例怎么构造？一般来说负例好选，通常就是随机选的。
2. 第二个关键问题是Encoder映射函数，这个映射函数怎么设计？这是个比较关键的问题。
3. 第三个问题是Loss function怎么设计？

### SimCLR

![image.png | 600](assets/image-20211114114151-mjnjjlk.png)

* In-Batch内随机负例：也就是说，在这个Batch里面除了我这个正例之外的任何其他例子都做为负例。
* 双塔结构相同，参数共享

### **Moco**


## 对比学习应用于推荐领域

不要忘记了“对比学习”的设计初衷。对比学习属于Self-Supervised Learning的一种，就是在没有标记样本的情况下进行学习。**推荐系统中有海量的用户反馈，那对比学习还有用武之地吗？**答案是肯定的

* 推荐日志中的item出现次数严重不均衡。对于cold item，用户的反馈依然是稀疏的。
* cold user的行为也是稀疏的。

对于cold user和item，我们不是完全没有办法。我们依然可以设计特征，喂入模型。但是由于模型已经被hot user/item的特征带偏了，所以即使我们喂入了cold user/item的特征，依然可能被模型所忽视。

所以一种思路是，遵循NLP中常见的“==pre-train+fine_tune==”的思路，

* 先用大量的未曝光数据将embedding+encoder部分训练好，训练出对new/cold user/item都比较友好的结构
* 再用click log里面的数据，对以上结构进行fine-tuning


## Trick

### ==对Embedding做Normalization更好==

在相似度计算时有两个选择：一种是用内积，一种是用cosine。如果你把cosine公式写出来仔细看下的话，会发现cosine可以理解为对user embedding和item embedding各自先做了一个L2 Norm,然后两者再做内积。**我们应该对user embedding和item embedding做Norm。意思就是要么你用cosine给它自动做Norm，要么用内积的话，原则上要在前一步做个L2 Norm，然后再去求内积，这两种做法基本等价。**