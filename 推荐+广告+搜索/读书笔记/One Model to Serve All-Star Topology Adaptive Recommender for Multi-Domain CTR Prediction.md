---
alias: STAR
å‘å¸ƒæ—¶é—´: 2021-11-02
å‡ºå“æ–¹: é˜¿é‡Œå¦ˆå¦ˆ
ä»·å€¼: â­â­â­
---

ç¯èŠ‚:: ç²¾æ’
å…³é”®è¯:: #Reco/å¤šåœºæ™¯æ¨è

---
# One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction

> STARç»™å‡ºäº†ä¸€ç§æ¯”è¾ƒç›´æ¥ä½†æœ‰æ•ˆçš„è§£æ³•ï¼Œå…·ä½“ç»†èŠ‚è¯·è§STARè®ºæ–‡ã€‚äº‹å®ä¸ŠSTATå¼€å¯çš„è§†è§’è¿œæ¯”å¤šåœºæ™¯å»ºæ¨¡å®å¤§ï¼Œå®ƒæŠŠæˆ‘ä»¬å¯¹é€šç”¨å»ºæ¨¡ä¸­ä¸€äº›ç»†å¾®çš„ã€å…ˆéªŒè®¤ä¸ºå…·å¤‡ç‰¹å¾é—´/æ ·æœ¬é—´å·®å¼‚æ€§çš„ä¿¡æ¯ï¼Œé€šè¿‡å¯¹åº”çš„è¾“å…¥ç‰¹å¾åšåˆ†å‰ï¼Œå¯¹åˆ†å‰çš„æ¯ä¸€éƒ¨åˆ†è®¾è®¡ç‹¬ç«‹çš„ç½‘ç»œç»“æ„ï¼ŒåŒæ—¶åˆ†å‰ç‚¹è¾“å…¥ç»™å…±äº«çš„ç½‘ç»œç»“æ„ã€‚è¿™æ ·å°±èƒ½å·§å¦™åœ°æŠŠdomain knowledgeå˜æˆç½‘ç»œç»“æ„ã€åœ¨è¾“å…¥ä¾§é€šè¿‡æŸäº›ç‰¹å¾ç»´åº¦å¢åŠ è‡ªç”±åº¦çš„å½¢å¼å¼•å…¥ã€‚ä¾‹å¦‚ï¼Œå¤šåœºæ™¯å»ºæ¨¡å°±æ˜¯åœ¨åœºæ™¯ç‰¹å¾ä¸Šå¢åŠ äº†è‡ªç”±åº¦


è¿™ç¯‡æ˜¯multi-domainï¼Œè€Œémulti-taskï¼Œæœ‰å…±é€šä¹‹å¤„ï¼Œä¹Ÿæœ‰åŒºåˆ«ã€‚æ‰€è°“domainï¼Œè¿™é‡Œæ˜¯æŒ‡æ·˜å®appä¸­ä¸åŒçš„åº”ç”¨åœºæ™¯ï¼Œæ¯”å¦‚å¤´éƒ¨çš„bannerï¼Œè¿˜æœ‰çŒœä½ å–œæ¬¢ã€‚é—®é¢˜çš„å…³é”®æ˜¯ï¼Œ**é¢å¯¹åŒä¸€ä¸ªlabelï¼Œä¸åŒdomainä¸‹çš„æ•°æ®åˆ†å¸ƒæœ‰å¾ˆå¤§ä¸åŒ**ã€‚è¿™ç¯‡æ–‡ç« ï¼Œå¯¹äºæˆ‘ä»¬è¿˜æ˜¯æœ‰å€Ÿé‰´æ„ä¹‰çš„ï¼Œmulti-domainå¯¹åº”multi-nationã€‚å½“ç„¶ï¼Œå¦‚æœåƒæˆ‘ä»¬è¿™ç§ï¼Œ**æ—¢æ˜¯multi-domainåˆæ˜¯multi-taskçš„åœºæ™¯ï¼Œç®€å•åœ°multi-domain * multi-taskè‚¯å®šæ˜¯åƒä¸æ¶ˆçš„**ã€‚

~~æ•´ä½“è¯„ä»·å¹¶ä¸é«˜~~ï¼Œæœ‰ç‚¹æ„æ€

* æ•´ä¸ªæ¶æ„ä¾èµ–äºç‰¹æ®Šçš„æ•°æ®æµï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªbatchå†…çš„æ ·æœ¬éƒ½æ¥è‡ªä¸€ä¸ªdomain
* ç»“æ„ä¸Šï¼Œæ˜¯æ‹¿**share weightå’Œdomain specific weightè¿›è¡Œelementwise multiplication**å¾—åˆ°æœ€ç»ˆç”¨äºå‰ä»£+å›ä»£çš„æƒé‡ã€‚è¿™ç§æ–¹å¼è¦æ±‚shareéƒ¨åˆ†ä¸domain specificéƒ¨åˆ†å¿…é¡»æœ‰å®Œå…¨ç›¸åŒçš„ç»“æ„ï¼Œè€Œä¸”çœ‹ä¸å‡ºä¸`final logit = share logit + domain-specific logit`çš„åŒºåˆ«ã€‚
* æ‹¿ä¸€ä¸ªç®€å•ç»“æ„ä¸“é—¨å¤„ç†domain indicatorï¼Œç„¶åç›´æ¥åŠ åˆ°final logitä¸­ï¼Œç¦»lossè¿‘ä¸€äº›ã€‚ç›®å‰çœ‹æ¥åªèƒ½ç®—æ˜¯å¸¸è§„æ“ä½œã€‚
* å¯èƒ½æœ€èµ·ä½œç”¨çš„ï¼Œè¿˜æ˜¯æ‰€è°“paritioned normalizationï¼Œä¹Ÿå°±æ˜¯æŒ‰domainè¿›è¡Œnormalizationã€‚è€Œå¦‚æœå®ç°èµ·æ¥ï¼Œä¹Ÿå¿…é¡»å’Œ"one batch, one domain"çš„æ•°æ®æµå¯†åˆ‡ç›¸å…³çš„ã€‚


in the proposed STAR model, 

* we let all business domains **share the same embedding layer,** ï¼ˆæœªå¿…æ˜¯æœ€ä¼˜çš„ï¼Œç°åœ¨æµè¡Œåˆ†ç¦»ï¼‰i.e., the same ID features in different domains share the same embedding.
* The embeddings are then pooled and concatenated to obtain ğµ fixed-length representations. After that, the ğµ extracted representations are processed by the proposed **partitioned normalization (PN) layer** that privatizes normalization statistics for different domains.
* The normalized vectors are then fed as input to the proposed star topology FCN to get the output. The star topology FCN consists of shared centered FCN and multiple domain-specific FCNs.
* æœ€åçš„è¾“å‡ºç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼ŒThe final model of each domain is obtained by combining 

  * **the shared centered FCN**
  * **domain-specific FCN**.
  * features that depict the domain information is of importance. In the STAR model, the auxiliary network treats the **domain indicator as input and fed with other features depicting the domain** to the auxiliary network. **The output of the auxiliary network is combined with the output of the star topology FCN to get the final prediction**.


## Partitioned Normalization

æ™®é€š"Batch Normalization"[^1]çš„å‡è®¾æ˜¯ï¼Œall samples are i.i.d. and use the shared statistics across all training samples. However, in multi-domain CTR prediction, **samples are only assumed to be locally i.i.d. within a specific domain**ã€‚

è®­ç»ƒçš„æ—¶å€™ï¼Œ**è¦æ±‚æŸä¸ªmini-batchæ˜¯å®Œå…¨æ¥è‡ªæŸä¸ªdomainçš„**ï¼Œè¿™æ—¶partitioned normalization=![image.png | 200](assets/image-20211126103651-m5al19n.png)

* ğ›¾ , ğ›½ are the global scale and bias,
* and $\gamma_p, \beta_p$ are the domainspecific scale and bias parameters.
* they are all learnable parameters

compared with BN, PN **also uses the moments of the current minibatch during training**, but PN introduces **domain-specific scale and bias** ğ›¾ ğ‘ , ğ›½ ğ‘ to capture the domain distinction.

during test, PN also let different domains to **accumulate the domain-specific moving average** of mean ğ¸ ğ‘ and varianceğ‘‰ğ‘ğ‘Ÿ ğ‘ . ![image.png | 200](assets/image-20211126104311-om0fu2z.png)


## ç½‘ç»œç»“æ„

å¾ˆæœ‰æ„æ€ã€å¾ˆæ–°é¢–çš„ä¸€ç§å°†shareçš„éƒ¨åˆ†ä¸domain-specificéƒ¨åˆ†ç»“åˆçš„æ–¹å¼ï¼Œä½†æ˜¯çœ‹ä¸å‡ºå“ªé‡Œå¥½ã€‚è¿™é‡Œçš„æ–¹å¼æ˜¯è¯´ï¼Œshareçš„éƒ¨åˆ†ä¸domain-specificçš„éƒ¨åˆ†å¿…é¡»æœ‰å®Œå…¨ç›¸åŒçš„ç»“æ„ï¼Œé‚£ä¹ˆå¯¹äºæŸä¸ªdomainï¼Œ**ç”¨äºå‰ä»£+å›ä»£çš„weightæ˜¯shareéƒ¨åˆ†ä¸domain-specificéƒ¨åˆ†çš„element-wiseç›¸ä¹˜**

  
![image.png | 200](assets/image-20211128105125-ba2hj9m.png)  
![image.png | 200](assets/image-20211227163139-wdmq5mv.png)

![image.png | 500](assets/image-20211128105146-7spr4jj.png)

* For each domain, the final weights of a neural network layer are obtained by element-wise multiplying the weights of the shared FCN and the domainspecific FCN.
* The shared parameters are updated through the gradient of all examples,
* while the domain-specific parameters are only updated through examples within this domain. ~~è¿™ä¸€ç‚¹æ˜¯é€šè¿‡æ¯ä¸ªbatchéƒ½åªæ¥è‡ªä¸€ä¸ªdomainæ¥ä¿è¯çš„ã€‚~~

ä¸€ä¸ªbatchå†…éƒ¨å¯ä»¥åŒ…å«å¤šåŸŸçš„æ•°æ®ï¼Œå¯ä»¥é€šè¿‡`tf.boolean_mask`æ¥å°†batchçš„æ•°æ®æ‹†åˆ†

```python

is_egy = config.get_label("is_egy")
is_kwaime = tf.squeeze(tf.equal(is_egy, 1), axis=1)
is_others = tf.squeeze(tf.equal(is_egy, 0), axis=1)


def split_by_app(inputs):
    """ æ ¹æ®å¸‚åœºåˆ‡ batch.
    Args:
        inputs (tensor): [batch_size, dim]
    """
    def split(inputs):
        indices = tf.range(tf.shape(inputs)[0], dtype=tf.int32)
        kwaime_inputs = tf.boolean_mask(inputs, is_kwaime, axis=0)
        kwaime_indices = tf.boolean_mask(indices, is_kwaime)
        others_inputs = tf.boolean_mask(inputs, is_others, axis=0)
        others_indices = tf.boolean_mask(indices, is_others)
        return kwaime_inputs, kwaime_indices, others_inputs, others_indices

    if inputs is None:
        return None, None, None, None
    elif isinstance(inputs, list):
        kwaime_inputs_list = []
        kwaime_indices_list = []
        others_inputs_list = []
        others_indices_list = []
        for one_inputs in inputs:
            a, b, c, d = split(one_inputs)
            kwaime_inputs_list.append(a)
            kwaime_indices_list.append(b)
            others_inputs_list.append(c)
            others_indices_list.append(d)
        return kwaime_inputs_list, kwaime_indices_list, others_inputs_list, others_indices_list
    else:
        return split(inputs)


def merge_by_app(kwaime_inputs, kwaime_indices, others_inputs, others_indices):
    inputs_merge = tf.concat([kwaime_inputs, others_inputs], axis=0)
    indices_merge = tf.concat([kwaime_indices, others_indices], axis=0)
    raw_indices = tf.contrib.framework.argsort(indices_merge)
    outputs = tf.gather(inputs_merge, raw_indices, axis=0)
    return outputs
```

ä½†æ˜¯~~çœ‹ä¸å‡ºæ¥è¿™ç§æ–¹å¼ï¼Œä¸shareå’Œdomain-specificå„æœ‰å„çš„ç»“æ„ï¼Œç„¶ådomain-logit = share-logit + domain-specific logitæœ‰ä»€ä¹ˆåŒºåˆ«ï¼ŸåŒæ ·èƒ½å¤Ÿåšåˆ°æ‰€æœ‰æ ·æœ¬éƒ½æ›´æ–°shareéƒ¨åˆ†ï¼Œåªæœ‰domain-specificæ ·æœ¬æ‰æ›´æ–°domain-specific networkçš„ç›®çš„ï¼Œè€Œä¸”å®ç°èµ·æ¥è¿˜æ›´ç®€å•ã€‚~~

å…¶å®ä¸¤ç§æ–¹å¼ï¼Œå®ç°èµ·æ¥ï¼Œè¿˜æ˜¯æœ‰æ¯”è¾ƒå¤§çš„åŒºåˆ«çš„ã€‚

* å¦‚æœé‡‡ç”¨parallel structureï¼Œåªæ˜¯æœ€åä¸¤ä¸ªlogitç›¸åŠ ï¼Œåªæ˜¯è¯´å„domainä¸Šçš„lossï¼Œæ—¢ä¼šå½±å“domain specific weightï¼Œä¹Ÿä¼šå½±å“shared weightã€‚ä½†æ˜¯, **shared towerä¸domain specific towerå„è‡ªåœ¨ä¿¡æ¯å‘ä¸Šä¼ é€’æ—¶æ˜¯å®Œå…¨ç‹¬ç«‹çš„**ã€‚
* å¦‚æœæˆ‘ä»¬æƒ³å®ç°ä¸€ç§ï¼Œæ¯ä¸€å±‚çš„è¾“å‡ºéƒ½ç›¸åŠ ä¸€ä¸‹ï¼Œå†é€šè¿‡activationçš„äº¤äº’æ–¹å¼ã€‚ä»¥ä¸Šé‚£ç§ä¸¤ç§logitç›¸åŠ ï¼Œå°±å®ç°ä¸äº†ã€‚ç›®å‰è¿™ç§domain-specific weightä¸shared weightæŒ‰ä½ç›¸ä¹˜çš„æ–¹å¼ï¼Œåè€Œèƒ½è®©äº¤å‰æ›´ç»†åŒ–ã€æ·±å…¥ä¸€äº›ã€‚


~~ä»¥ä¸Šå…¬å¼æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯å¦‚ä½•ç”¨TFæ¥å®ç°ï¼Ÿï¼Ÿï¼Ÿ~~**å…¶å®ä¹Ÿå¥½å®ç°ï¼Œå°±åƒè®ºæ–‡é‡Œé¢é‚£æ ·ï¼Œå®šä¹‰ä¸¤å¥—weightç„¶åå†æŒ‰ä½ç›¸ä¹˜å°±è¡Œ**

è¿™ç§ä¸åŒæ¨¡å—çš„weightè¦fusionçš„ä½œæ³•ï¼Œå’Œåä¸º"Bridge Module"[^2]çš„ä½œæ³•æœ‰ç‚¹ç±»ä¼¼ï¼Œéš¾é“æ˜¯ä¸€ç§è¶‹åŠ¿ï¼Ÿï¼Ÿè€Œä¸”è¿™é‡Œä¸¤ä¸ªweightçš„èåˆä¹Ÿç”¨element-wise multiplicationï¼Œä¸Bridge Moduleä¸­çš„æœ€ä½³å®è·µä¹Ÿç›¸åŒã€‚


ä¸€å¼€å§‹æƒ³æƒ³ï¼Œè§‰å¾—è¿™ç§ä½œæ³•ç±»ä¼¼å¾®ä¿¡çš„"Personalized Transfer of User Preferences for Cross-domain Recom..."[^3]ï¼Œ~~æœ‰ç‚¹ç±»ä¼¼meta network~~ã€‚åæ¥å†æƒ³æƒ³ï¼Œä¸å¯¹ã€‚

* meta networkæ˜¯æŒ‡ï¼Œæ¯ä¸ªæ ·æœ¬ç»è¿‡çš„dense parameteræ˜¯ä¸ªæ€§åŒ–çš„ï¼Œ
* è€ŒSTARæ˜¾ç„¶ä¸æ˜¯ï¼Œæ¯ä¸ªåŸŸçš„æ ·æœ¬è¦ç»è¿‡çš„dense parameterè™½ç„¶æ˜¯domain specificå’Œshared weightçš„èåˆç»“æœï¼Œä½†æ˜¯ä¸¤ä¸ªweightè°ä¹Ÿä¸æ˜¯personalizedçš„

ä½†æ˜¯æœªæ¥æœªå¿…ä¸èƒ½å‘é‚£ä¸ªæ–¹å‘å»è¿ˆè¿›ã€‚


## Auxiliary Network

è¯´ç™½äº†ï¼Œå°±æ˜¯æ‹¿domain indicator embeddingå’Œå…¶ä»–ä¸€äº›å¼ºçƒˆè¡¨å¾domainçš„ç‰¹å¾ï¼Œæ‹¼æ¥ä¸€èµ·ï¼Œå†ç»è¿‡ä¸€ä¸ªç®€å•ç½‘ç»œï¼ˆThe simple architecture makes the domain features directly influence the final predictionï¼‰ï¼Œå¾—åˆ°ä¸€ä¸ªlogitï¼ˆç±»ä¼¼Wide & Deepï¼‰ã€‚æœ€ç»ˆçš„logitæ˜¯æ­£å¼logitå’Œè¿™ä¸ªauxilary logitä¹‹å’Œï¼Œå†è¿‡sigmoidã€‚**æ²¡æœ‰é’ˆå¯¹è¿™ä¸ªauxiliary logitå•ç‹¬åšauxiliary loss**ï¼Œå¯èƒ½åªæœ‰domain indicatoré‡Œé¢çš„ä¿¡æ¯æœ‰é™å§ã€‚