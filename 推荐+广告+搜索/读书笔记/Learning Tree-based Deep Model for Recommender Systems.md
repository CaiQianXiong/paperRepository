---
alias: TDM 1.0
å‘å¸ƒæ—¶é—´: 2018-12-21
å‡ºå“æ–¹: é˜¿é‡Œå¦ˆå¦ˆ
ä»·å€¼: â­â­â­
---

ç¯èŠ‚:: å¬å›
å…³é”®è¯:: 
URL:: [zotero](zotero://open-pdf/0_CTFARB6B/1)

---

# [Learning Tree-based Deep Model for Recommender Systems](https://arxiv.org/abs/1801.02294) 
[å®˜æ–¹demoå®ç°](https://github.com/alibaba/x-deeplearning/wiki/%E6%B7%B1%E5%BA%A6%E6%A0%91%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B(TDM))

``` ad-warning
title: å®æˆ˜èµ·æ¥ï¼Œé—®é¢˜å¤šå¤š
å‚è€ƒ[[ä¸‰é—®TDM]]
```

We propose a novel tree-based method which can provide logarithmic complexity w.r.t. corpus size even with more expressive models such as DNN.

* The main idea is to predict user interests from coarse to ï¬ne by traversing tree nodes in a top-down fashion and making decisions for each user-node pair. å®é™…ä¸Štreeçš„ä½œç”¨å°±æ˜¯ï¼Œå……å½“å…¨åº“itemçš„ç´¢å¼•ï¼Œç¼©å°æ£€ç´¢èŒƒå›´ï¼Œæ²¡å¿…è¦å…¨åº“é€ä¸€æ£€éªŒã€‚å¦‚æœç”¨æˆ·å·²ç»å¯¹æŸä¸ªcoarseå€™é€‰èŠ‚ç‚¹ä¸æ„Ÿå…´è¶£äº†ï¼Œå°±æ²¡æœ‰å¿…è¦æ²¿è¿™ä¸ªcoarseèŠ‚ç‚¹å‘ä¸‹ç»§ç»­æœç´¢äº†ã€‚
* the tree structure can be jointly learnt towards better compatibility with usersâ€™ interest distribution and hence facilitate both training and prediction

æ‰€è¦è§£å†³çš„é—®é¢˜å°±æ˜¯ï¼ŒConventionally, advanced expressive models can not be regulated to inner product form between user and item vectors to utilize eï¬ƒcient approximate kNN search, they can not be used to recall candidates in large-scale recommender systems.

**æ ‘ç»“æ„**

* each item in the corpus C corresponds to one and only one leaf node in the tree, and
* those non-leaf nodes are coarse-grained concepts.
* max-heap like tree, i.e., every non-leaf node `n` in level`j`satisï¬es the following equation for **each user u (???éš¾é“æ ‘çš„ç»“æ„æ˜¯å› userè€Œå¼‚å—ï¼Ÿ)**. It says that a parent nodeâ€™s **ground truth** preference equals to the maximum preference of its children nodes, divided by the normalization term.

![image.png](assets/image-20211227201247-2fhicic.png)

---

## Training

### å­¦ä¹ æ¨¡å‹

#### æ­£è´Ÿæ ·æœ¬

* Leaf node that have positive interaction with u, and its ancestor nodes constitute the set of **positive** samples in each level for u.
* Randomly selected nodes except positive ones in each level constitute the set of **negative** samples.

![image.png](assets/image-20211227201904-641mtdo.png)

These samples are then fed into binary probability models (æ‰€ä»¥è¿™é‡Œæ‰€è¯´çš„è´Ÿé‡‡æ ·ä¸approximate softmaxä¸­çš„negative samplingä¸åŒï¼Œå•çº¯é‡‡æ ·è€Œå·²ï¼Œä¸æ¶‰åŠå¯¹softmaxçš„ä¿®æ­£) to get levelsâ€™ order discriminators. We use one **global DNN** binary model for all levelsâ€™ order discriminators.

We **randomly select negative samples in the same level for each positive node**.

* Such method makes each levelâ€™s discriminator be an intra-level global one. Each levelâ€™s global discriminator can make precise decisions **independently, without depending on the goodness of upper levelsâ€™ decisions**.
* It ensures that even if the model makes bad decision and low quality nodes leak into the candidate set in an upper-level, those relatively better nodes rather than very bad ones can be chosen by the model in the following levels.


#### å¯¹äºæŸæ£µæ ‘æ—¶è®­ç»ƒæ¨¡å‹

å¯¹äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¯ä¸€æ£µæ ‘ï¼ˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸­é—´çŠ¶æ€ï¼‰ï¼Œéƒ½è¦è®­ç»ƒå¦‚ä¸‹çš„æ¨¡å‹

![image.png | 800](assets/image-20211227202653-omltm4l.png)

* æ¯ä¸ªitemåªèƒ½ç”¨item idæ¥å½“ç‰¹å¾ï¼Œå­¦ä¹ å…¶item id embeddingï¼Œ**è€Œä¸èƒ½ç”¨side information**ã€‚å› ä¸ºéå¶èŠ‚ç‚¹æ˜¯æŠ½è±¡çš„ï¼Œä¸å¯¹åº”ä»»ä½•å…·ä½“itemï¼Œéå¶èŠ‚ç‚¹ä¸Šçš„side informationæ²¡æ³•å¡«ã€‚
  * **å…¶å®ä¹Ÿä¸æ˜¯å®Œå…¨ä¸è¡Œã€‚å¯ä»¥æ‹¿å¶å­èŠ‚ç‚¹çš„side informationï¼Œé€å±‚å‘ä¸Šå †ç§¯ã€‚æ¯”å¦‚éå¶èŠ‚ç‚¹çš„tagæ˜¯å¶å­èŠ‚ç‚¹çš„tagçš„é›†åˆ**ã€‚ğŸ’¡
* äº¤å‰ç‰¹å¾ä¹Ÿä¸èƒ½ç”¨ï¼ˆï¼Ÿä¸ç¡®å®šï¼‰ï¼Œçº¿ä¸ŠæŠ½å–äº¤å‰ç‰¹å¾å¤ªè´¹åŠ²äº†å§
* é‡è¦ç‰¹å¾æ˜¯ç”¨æˆ·çš„user action listï¼Œè€Œä¸”è¿˜æŒ‰æ—¶é—´åˆ’åˆ†æˆäº†**è‹¥å¹²time window**
  * æ¯ä¸ªwindowä¸­çš„action item embeddingä¸node embeddingæ˜¯å¤ç”¨å…³ç³»å—ï¼Ÿå¯èƒ½æ˜¯ï¼Œå¯èƒ½ä¸æ˜¯ï¼Œçœ‹å®éªŒå§ã€‚
* æ¯ä¸ªtime windowä¸­çš„user action itemsä¸target itemåšattentionï¼Œä»è€Œå°†**æ¯ä¸ªtime windowåŠ æƒå¹³å‡æˆä¸€ä¸ªembedding**
* Each time windowâ€™s output along with the candidate nodeâ€™s embedding are concatenated as the neural network input

è®­ç»ƒé‡æ˜¯æå¤§çš„ï¼Œæ‰€ä»¥é˜¿é‡Œä¹Ÿåªèƒ½ç”¨ä¸€å¤©çš„æ•°æ®æ¥è®­ç»ƒ
``` ad-question
title: å²‚ä¸æ˜¯åªèƒ½item idå½“ç‰¹å¾ï¼Ÿ

å› ä¸ºå¦‚æœç”¨äº†å…¶ä»–éidç±»çš„ç‰¹å¾ï¼Œé‚£ä¹ˆéå¶èŠ‚ç‚¹æ€ä¹ˆåŠï¼Ÿéå¶èŠ‚ç‚¹ä¸æ˜¯å®ä½“ï¼Œå®ƒä»¬ä¸å¯èƒ½æœ‰é™¤äº†idä¹‹å¤–çš„å…¶ä»–ç‰¹å¾

æœ‰ä¸€ç§è¯´æ³•æ˜¯ï¼Œå¯ä»¥é€šè¿‡å¶å­èŠ‚ç‚¹çš„ç‰¹å¾å‘ä¸Šèšåˆï¼Œæ¯”å¦‚éå¶å­èŠ‚ç‚¹çš„æ ‡ç­¾æ˜¯å…¶æ‰€æœ‰å­èŠ‚ç‚¹çš„æ ‡ç­¾çš„é›†åˆï¼Œè¿™æ ·åšæ˜¯æœ‰ç‚¹ç²—ï¼Œä¹Ÿéº»çƒ¦
```

``` ad-question
title: ä¸­é—´éå¶èŠ‚ç‚¹çš„embeddingä»å“ªé‡Œæ¥ï¼Ÿ

æœ€å…³é”®çš„é—®é¢˜æ˜¯ä¸­é—´éå¶èŠ‚ç‚¹æ˜¯dynamic,virtualçš„ï¼Œä¸å…·å¤‡è¿ç»­è®­ç»ƒçš„ç»§æ‰¿æ€§

å¶å­èŠ‚ç‚¹éƒ½æ˜¯item idï¼ŒæŠŠembeddingå­˜å‚¨ä¸‹æ¥ï¼Œä¸‹æ¬¡å¯ä»¥loadè¿›æ¥ï¼Œç»§ç»­è®­ç»ƒ

ä½†æ˜¯ä¸­é—´èŠ‚ç‚¹å‘¢ï¼Ÿæ€»ä¸èƒ½å› ä¸ºå› ä¸ºå‰åä¸¤æ¬¡è®­ç»ƒï¼Œéƒ½ç¼–å·æˆâ€œç¬¬2å±‚ç¬¬3ä¸ªèŠ‚ç‚¹â€å°±ç»§æ‰¿ä¸Šæ¬¡çš„embeddingå§ï¼Œä½ç½®ç›¸åŒï¼Œå¹¶ä¸ä»£è¡¨èƒŒåçš„å®ä½“æ„ä¹‰æ˜¯ç›¸åŒçš„
```

``` ad-question
title: å¯¹äºä¸åŒå±‚çš„æ­£è´ŸèŠ‚ç‚¹ï¼Œæƒé‡éƒ½æ˜¯ä¸€æ ·çš„ï¼Ÿï¼Ÿï¼Ÿä¸€è§†åŒä»ï¼Ÿ

æ˜¾ç„¶ä¸å¯¹å‘€ï¼Œå¦‚æœä¸Šè¾¹åˆ†é”™äº†ï¼Œæ£€ç´¢æ—¶ä¸‹è¾¹å°±æ ¹æœ¬æ²¡æœºä¼šæ£€ç´¢åˆ°
æ˜¾ç„¶è¶Šå‘ä¸Šï¼Œåˆ†é”™çš„ä»£ä»·è¶Šå¤§ï¼Œåº”è¯¥ç»™äºˆæ›´å¤§çš„æƒé‡

è€Œä¸”å¦‚ä½•é¿å…çˆ¶äº²åˆ†é”™äº†ï¼Œè€Œå„¿å­åˆ†å¯¹äº†ï¼Œè¿™ç§ç°å®è’å”ï¼Œè€Œè®­ç»ƒæ—¶æœ‰å¯èƒ½å‘ç”Ÿçš„ç°è±¡ï¼Ÿï¼Ÿï¼Ÿ
```

### å­¦ä¹ æ ‘ç»“æ„

#### åˆå§‹åŒ–

itâ€™s natural to build the tree in a way that similar items are organized in close positions. Alibaba leverages itemâ€™s **category information to build the initial tree**.

* Firstly, we sort all categories randomly, and
  * place items belonging to the same category together in an intra-category random order.
  * If an item belongs to more than one category, the item is assigned to a random one for uniqueness.
  * ç¬¬ä¸€å±‚å°±æ˜¯ä¸€ä¸ªæ ¹èŠ‚ç‚¹ï¼Œç„¶åç¬¬äºŒå±‚æœ‰Mä¸ªèŠ‚ç‚¹ï¼ŒMæ˜¯æ€»çš„categoryæ•°ç›®
* Secondly, those ranked items are halved to two equal parts recursively until the current set contains only one item


åœ¨å®é™…åœºæ™¯ä¸‹ï¼Œ**å„item embeddingæ— éœ€ä»0éšæœºåˆå§‹åŒ–ï¼Œæœ‰å„ç§å…¶ä»–ç®—æ³•ï¼ˆword2vec, åŒå¡”ï¼‰èƒ½å¤Ÿæä¾›å·²ç»éå¸¸é è°±çš„item embeddingäº†**ï¼Œç”¨è¿™äº›item embeddingå»ºæˆçš„æ ‘ï¼Œå®é™…ä¸Šå°±å·²ç»æ˜¯æœ€ç»ˆçš„æ ‘äº†ï¼Œ**æ— éœ€å¤šæ¬¡å»ºæ ‘**ã€‚


#### å­¦ä¹ æ–°æ ‘

Learning each leaf nodeâ€™s embedding can be learnt after model training. Then we use the learnt leaf nodesâ€™ embedding vectors to cluster a new tree.

* After a DNN is learnt, items are clustered into two subsets according to their embedding vectors, via K-means.
* the two subsets are **adjusted** to equal for a more balanced tree.
* The recursion stops when only one item is left, and a binary tree could be constructed in such a top-down way.


### äº¤æ›¿å­¦ä¹ 

![image.png | 300](assets/image-20211227205804-8cl8jk8.png)

The deep model and tree structure are learnt ~~jointly in an alternative way~~ï¼ˆå®é™…åœºæ™¯ä¸‹å¹¶ä¸éœ€è¦ï¼‰:

1. Construct an initial tree and train the model till converging;
2. Learn to get a new tree structure in basis of trained leaf nodesâ€™ embeddings;
3. æ ¹æ®æ–°æ ‘ç»“æ„ï¼Œé‡æ–°å®šä¹‰æ­£è´Ÿæ ·æœ¬
4. Train the model again with the learnt new tree structure.

* è€Œå®é™…å·¥ä½œä¸­ï¼Œ**æ— éœ€åœ¨å­¦ä¹ æ ‘ç»“æ„ä¸å­¦ä¹ æ¨¡å‹ä¹‹é—´å¤šæ¬¡äº¤æ›¿è¿­ä»£**
* ä»å…¶ä»–å¬å›ç®—æ³•ä¸­å·²ç»èƒ½å¤Ÿè·å¾—éå¸¸å¥½çš„item embeddingäº†ï¼Œç”¨è¿™äº›item embeddingå°±èƒ½å¤Ÿå»ºæ ‘äº†ï¼Œè€Œä¸”å»ºæˆçš„æ ‘å°±å·²ç»é è°±äº†ï¼Œ**å°±å¯ä»¥åœ¨ä¸€æ®µæ—¶é—´å†…å›ºå®šä¸‹æ¥äº†ï¼ˆe.g., ä¸€å¤©ï¼‰è€Œæ— éœ€åå¤é‡å»º**
* ä¸€å¤©ä¹‹å†…ï¼Œæ ‘å¯ä»¥æ˜¯åŸºæœ¬å›ºå®šçš„ã€‚ä¹‹æ‰€ä»¥æ˜¯åŸºæœ¬ï¼Œæ˜¯å› ä¸ºåœ¨è¿™ä¸€å¤©ä¹‹å†…æ–°å‡ºç°çš„itemï¼Œå¯ä»¥é€šè¿‡å…¶ä»–ç®—æ³•æä¾›çš„item embeddingï¼Œè¢«æ’å…¥åœ¨å½“å‰æ ‘ä¸­ã€‚æ’å…¥åï¼Œéœ€è¦é‡æ–°é‡‡æ ·æ­£è´Ÿæ ·æœ¬ï¼Œé‡æ–°è®­ç»ƒæ¨¡å‹ã€‚

## Serving

For level `l`, only the children of nodes with top-k probabilities in level `l âˆ’ 1` are scored and sorted to pick k candidate nodes in level l.

![image.png | 500](assets/image-20211227201524-rqmeys6.png)